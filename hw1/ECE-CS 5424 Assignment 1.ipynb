{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE-5424 / CS-5824 Advanced Machine Learning\n",
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, **you need to complete the following four sectoins**:\n",
    "1. KNN\n",
    "2. Linear regression\n",
    "3. Logistic regression\n",
    "4. Regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission guideline\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Please make sure to have entered your Virginia Tech PID below.\n",
    "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of ll cells).\n",
    "4. Select Cell -> Run All. This will run all the cells in order.\n",
    "5. Once you've rerun everything, select File -> Download as -> PDF via LaTeX\n",
    "6. Look at the PDF file and make sure all your solutions are there, displayed correctly. \n",
    "7. Upload the PDF file and this notebook **INDEPENDENTLY**.\n",
    "8. Please **DO NOT** uplaod any data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please Write Your VT PID Here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. KNN [25 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following KNN assignment is modified from [Stanford CS231n](http://cs231n.stanford.edu/). Please complete and hand in this completed worksheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "from data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data:\n",
    "Once you have the starter code (regardless of which method you choose above), you will need to download the CIFAR-10 dataset. Run the following from the assignment1 directory:\n",
    "\n",
    "```cd data\n",
    "./get_datasets.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "cifar10_dir = 'data/cifar-10-batches-py'\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample the data for more efficient code execution in this exercise\n",
    "num_training = 5000\n",
    "mask = list(range(num_training))\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "num_test = 500\n",
    "mask = list(range(num_test))\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things much structural, we now put everything together into the KNearestNeighbor class. You don't need to implement any fucntion in this class now. Later you will need to come back here and implement the asked function, per the instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbor(object):\n",
    "  \"\"\" a kNN classifier with L2 distance \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def train(self, X, y):\n",
    "    \"\"\"\n",
    "    Train the classifier. For k-nearest neighbors this is just \n",
    "    memorizing the training data.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (num_train, D) containing the training data\n",
    "      consisting of num_train samples each of dimension D.\n",
    "    - y: A numpy array of shape (N,) containing the training labels, where\n",
    "         y[i] is the label for X[i].\n",
    "    \"\"\"\n",
    "    self.X_train = X\n",
    "    self.y_train = y\n",
    "    \n",
    "  def predict(self, X, k=1, num_loops=0):\n",
    "    \"\"\"\n",
    "    Predict labels for test data using this classifier.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (num_test, D) containing test data consisting\n",
    "         of num_test samples each of dimension D.\n",
    "    - k: The number of nearest neighbors that vote for the predicted labels.\n",
    "    - num_loops: Determines which implementation to use to compute distances\n",
    "      between training points and testing points.\n",
    "\n",
    "    Returns:\n",
    "    - y: A numpy array of shape (num_test,) containing predicted labels for the\n",
    "      test data, where y[i] is the predicted label for the test point X[i].  \n",
    "    \"\"\"\n",
    "    if num_loops == 0:\n",
    "      dists = self.compute_distances_no_loops(X)\n",
    "    elif num_loops == 1:\n",
    "      dists = self.compute_distances_one_loop(X)\n",
    "    elif num_loops == 2:\n",
    "      dists = self.compute_distances_two_loops(X)\n",
    "    else:\n",
    "      raise ValueError('Invalid value %d for num_loops' % num_loops)\n",
    "\n",
    "    return self.predict_labels(dists, k=k)\n",
    "\n",
    "  def compute_distances_two_loops(self, X):\n",
    "    \"\"\"\n",
    "    Compute the distance between each test point in X and each training point\n",
    "    in self.X_train using a nested loop over both the training data and the \n",
    "    test data.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (num_test, D) containing test data.\n",
    "\n",
    "    Returns:\n",
    "    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n",
    "      is the Euclidean distance between the ith test point and the jth training\n",
    "      point.\n",
    "    \"\"\"\n",
    "    num_test = X.shape[0]\n",
    "    num_train = self.X_train.shape[0]\n",
    "    dists = np.zeros((num_test, num_train))\n",
    "    for i in range(num_test):\n",
    "      for j in range(num_train):\n",
    "        #####################################################################\n",
    "        # TODO:                                                             #\n",
    "        # Compute the l2 distance between the ith test point and the jth    #\n",
    "        # training point, and store the result in dists[i, j]. You should   #\n",
    "        # not use a loop over dimension.                                    #\n",
    "        #####################################################################\n",
    "        pass\n",
    "        #####################################################################\n",
    "        #                       END OF YOUR CODE                            #\n",
    "        #####################################################################\n",
    "    return dists\n",
    "\n",
    "  def compute_distances_one_loop(self, X):\n",
    "    \"\"\"\n",
    "    Compute the distance between each test point in X and each training point\n",
    "    in self.X_train using a single loop over the test data.\n",
    "\n",
    "    Input / Output: Same as compute_distances_two_loops\n",
    "    \"\"\"\n",
    "    num_test = X.shape[0]\n",
    "    num_train = self.X_train.shape[0]\n",
    "    dists = np.zeros((num_test, num_train))\n",
    "    for i in range(num_test):\n",
    "      #######################################################################\n",
    "      # TODO:                                                               #\n",
    "      # Compute the l2 distance between the ith test point and all training #\n",
    "      # points, and store the result in dists[i, :].                        #\n",
    "      #######################################################################\n",
    "      pass\n",
    "      #######################################################################\n",
    "      #                         END OF YOUR CODE                            #\n",
    "      #######################################################################\n",
    "    return dists\n",
    "\n",
    "  def compute_distances_no_loops(self, X):\n",
    "    \"\"\"\n",
    "    Compute the distance between each test point in X and each training point\n",
    "    in self.X_train using no explicit loops.\n",
    "\n",
    "    Input / Output: Same as compute_distances_two_loops\n",
    "    \"\"\"\n",
    "    num_test = X.shape[0]\n",
    "    num_train = self.X_train.shape[0]\n",
    "    dists = np.zeros((num_test, num_train)) \n",
    "    #########################################################################\n",
    "    # TODO:                                                                 #\n",
    "    # Compute the l2 distance between all test points and all training      #\n",
    "    # points without using any explicit loops, and store the result in      #\n",
    "    # dists.                                                                #\n",
    "    #                                                                       #\n",
    "    # You should implement this function using only basic array operations; #\n",
    "    # in particular you should not use functions from scipy.                #\n",
    "    #                                                                       #\n",
    "    # HINT: Try to formulate the l2 distance using matrix multiplication    #\n",
    "    #       and two broadcast sums.                                         #\n",
    "    #########################################################################\n",
    "    pass\n",
    "\n",
    "    #########################################################################\n",
    "    #                         END OF YOUR CODE                              #\n",
    "    #########################################################################\n",
    "    return dists\n",
    "\n",
    "  def predict_labels(self, dists, k=1):\n",
    "    \"\"\"\n",
    "    Given a matrix of distances between test points and training points,\n",
    "    predict a label for each test point.\n",
    "\n",
    "    Inputs:\n",
    "    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n",
    "      gives the distance betwen the ith test point and the jth training point.\n",
    "\n",
    "    Returns:\n",
    "    - y: A numpy array of shape (num_test,) containing predicted labels for the\n",
    "      test data, where y[i] is the predicted label for the test point X[i].  \n",
    "    \"\"\"\n",
    "    num_test = dists.shape[0]\n",
    "    y_pred = np.zeros(num_test)\n",
    "    for i in range(num_test):\n",
    "      # A list of length k storing the labels of the k nearest neighbors to\n",
    "      # the ith test point.\n",
    "\n",
    "      #########################################################################\n",
    "      # TODO:                                                                 #\n",
    "      # Use the distance matrix to find the k nearest neighbors of the ith    #\n",
    "      # testing point, and use self.y_train to find the labels of these       #\n",
    "      # neighbors. Store these labels in closest_y.                           #\n",
    "      # Hint: Look up the function numpy.argsort.                             #\n",
    "      #########################################################################\n",
    "      pass\n",
    "      #########################################################################\n",
    "      # TODO:                                                                 #\n",
    "      # Now that you have found the labels of the k nearest neighbors, you    #\n",
    "      # need to find the most common label in the list closest_y of labels.   #\n",
    "      # Store this label in y_pred[i]. Break ties by choosing the smaller     #\n",
    "      # label.                                                                #\n",
    "      #########################################################################\n",
    "      pass\n",
    "      #########################################################################\n",
    "      #                           END OF YOUR CODE                            # \n",
    "      #########################################################################\n",
    "\n",
    "    return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a kNN classifier instance. \n",
    "# Remember that training a kNN classifier is a noop: \n",
    "# the Classifier simply remembers the data and does no further processing \n",
    "classifier = KNearestNeighbor()\n",
    "classifier.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now like to classify the test data with the kNN classifier. Recall that we can break down this process into two steps: \n",
    "\n",
    "1. First we must compute the distances between all test examples and all train examples. \n",
    "2. Given these distances, for each test example we find the k nearest examples and have them vote for the label\n",
    "\n",
    "Lets begin with computing the distance matrix between all training and test examples. For example, if there are **Ntr** training examples and **Nte** test examples, this stage should result in a **Nte x Ntr** matrix where each element (i,j) is the distance between the i-th test and j-th train example.\n",
    "\n",
    "First, open `k_nearest_neighbor.py` and implement the function `compute_distances_two_loops` that uses a (very inefficient) double loop over all pairs of (test, train) examples and computes the distance matrix one element at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement compute_distances_two_loops.\n",
    "\n",
    "# Test your implementation:\n",
    "dists = classifier.compute_distances_two_loops(X_test)\n",
    "print(dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualize the distance matrix: each row is a single test example and\n",
    "# its distances to training examples\n",
    "plt.imshow(dists, interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question #1:** Notice the structured patterns in the distance matrix, where some rows or columns are visible brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.)\n",
    "\n",
    "- What in the data is the cause behind the distinctly bright rows?\n",
    "- What causes the columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**: *fill this in.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, open k_nearest_neighbor.py and implement the function predict_labels that predicts a label for each test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now implement the function predict_labels and run the code below:\n",
    "# We use k = 1 (which is Nearest Neighbor).\n",
    "y_test_pred = classifier.predict_labels(dists, k=1)\n",
    "\n",
    "# Compute and print the fraction of correctly predicted examples\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = float(num_correct) / num_test\n",
    "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect to see approximately `27%` accuracy. Now lets try out a larger `k`, say `k = 5`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = classifier.predict_labels(dists, k=5)\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = float(num_correct) / num_test\n",
    "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question 2**\n",
    "We can also try other distance metrics such as L1 distance.\n",
    "The performance of a Nearest Neighbor classifier that uses L1 distance will not change if (Select all that apply.):\n",
    "1. The data is preprocessed by subtracting the mean.\n",
    "2. The data is preprocessed by subtracting the mean and dividing by the standard deviation.\n",
    "3. The coordinate axes for the data are rotated.\n",
    "4. None of the above.\n",
    "\n",
    "*Your Answer*:\n",
    "\n",
    "*Your explanation*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets speed up distance matrix computation by using partial vectorization\n",
    "# with one loop. Implement the function compute_distances_one_loop and run the\n",
    "# code below:\n",
    "dists_one = classifier.compute_distances_one_loop(X_test)\n",
    "\n",
    "# To ensure that our vectorized implementation is correct, we make sure that it\n",
    "# agrees with the naive implementation. There are many ways to decide whether\n",
    "# two matrices are similar; one of the simplest is the Frobenius norm. In case\n",
    "# you haven't seen it before, the Frobenius norm of two matrices is the square\n",
    "# root of the squared sum of differences of all elements; in other words, reshape\n",
    "# the matrices into vectors and compute the Euclidean distance between them.\n",
    "difference = np.linalg.norm(dists - dists_one, ord='fro')\n",
    "print('Difference was: %f' % (difference, ))\n",
    "if difference < 0.001:\n",
    "    print('Good! The distance matrices are the same')\n",
    "else:\n",
    "    print('Uh-oh! The distance matrices are different')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now implement the fully vectorized version inside compute_distances_no_loops\n",
    "# and run the code\n",
    "dists_two = classifier.compute_distances_no_loops(X_test)\n",
    "\n",
    "# check that the distance matrix agrees with the one we computed before:\n",
    "difference = np.linalg.norm(dists - dists_two, ord='fro')\n",
    "print('Difference was: %f' % (difference, ))\n",
    "if difference < 0.001:\n",
    "    print('Good! The distance matrices are the same')\n",
    "else:\n",
    "    print('Uh-oh! The distance matrices are different')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare how fast the implementations are\n",
    "def time_function(f, *args):\n",
    "    \"\"\"\n",
    "    Call a function f with args and return the time (in seconds) that it took to execute.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    tic = time.time()\n",
    "    f(*args)\n",
    "    toc = time.time()\n",
    "    return toc - tic\n",
    "\n",
    "two_loop_time = time_function(classifier.compute_distances_two_loops, X_test)\n",
    "print('Two loop version took %f seconds' % two_loop_time)\n",
    "\n",
    "one_loop_time = time_function(classifier.compute_distances_one_loop, X_test)\n",
    "print('One loop version took %f seconds' % one_loop_time)\n",
    "\n",
    "no_loop_time = time_function(classifier.compute_distances_no_loops, X_test)\n",
    "print('No loop version took %f seconds' % no_loop_time)\n",
    "\n",
    "# you should see significantly faster performance with the fully vectorized implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "We have implemented the k-Nearest Neighbor classifier but we set the value k = 5 arbitrarily. We will now determine the best value of this hyperparameter with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]\n",
    "\n",
    "X_train_folds = []\n",
    "y_train_folds = []\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Split up the training data into folds. After splitting, X_train_folds and    #\n",
    "# y_train_folds should each be lists of length num_folds, where                #\n",
    "# y_train_folds[i] is the label vector for the points in X_train_folds[i].     #\n",
    "# Hint: Look up the numpy array_split function.                                #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "# A dictionary holding the accuracies for different values of k that we find\n",
    "# when running cross-validation. After running cross-validation,\n",
    "# k_to_accuracies[k] should be a list of length num_folds giving the different\n",
    "# accuracy values that we found when using that value of k.\n",
    "k_to_accuracies = {}\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Perform k-fold cross validation to find the best value of k. For each        #\n",
    "# possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #\n",
    "# where in each case you use all but one of the folds as training data and the #\n",
    "# last fold as a validation set. Store the accuracies for all fold and all     #\n",
    "# values of k in the k_to_accuracies dictionary.                               #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "# Print out the computed accuracies\n",
    "for k in sorted(k_to_accuracies):\n",
    "    for accuracy in k_to_accuracies[k]:\n",
    "        print('k = %d, accuracy = %f' % (k, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the raw observations\n",
    "for k in k_choices:\n",
    "    accuracies = k_to_accuracies[k]\n",
    "    plt.scatter([k] * len(accuracies), accuracies)\n",
    "\n",
    "# plot the trend line with error bars that correspond to standard deviation\n",
    "accuracies_mean = np.array([np.mean(v) for k,v in sorted(k_to_accuracies.items())])\n",
    "accuracies_std = np.array([np.std(v) for k,v in sorted(k_to_accuracies.items())])\n",
    "plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)\n",
    "plt.title('Cross-validation on k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Cross-validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the cross-validation results above, choose the best value for k,   \n",
    "# retrain the classifier using all the training data, and test it on the test\n",
    "# data. You should be able to get above 28% accuracy on the test data.\n",
    "best_k = 1\n",
    "\n",
    "classifier = KNearestNeighbor()\n",
    "classifier.train(X_train, y_train)\n",
    "y_test_pred = classifier.predict(X_test, k=best_k)\n",
    "\n",
    "# Compute and display the accuracy\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = float(num_correct) / num_test\n",
    "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question 3**\n",
    "Which of the following statements about $k$-Nearest Neighbor ($k$-NN) are true in a classification setting, and for all $k$? Select all that apply.\n",
    "1. The training error of a 1-NN will always be better than that of 5-NN.\n",
    "2. The test error of a 1-NN will always be better than that of a 5-NN.\n",
    "3. The decision boundary of the k-NN classifier is linear.\n",
    "4. The time needed to classify a test example with the k-NN classifier grows with the size of the training set.\n",
    "5. None of the above.\n",
    "\n",
    "*Your Answer*:\n",
    "\n",
    "*Your explanation*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Linear Regression [25 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following linear regression assignment is modified from [Stanford CS229](http://cs229.stanford.edu//). Please complete and hand in this completed worksheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting on any task, it is often useful to understand the data by visualizing it. For this dataset, you can use a scatter plot to visualize the data, since it has only two properties to plot (profit and population). (Many other problems that you will encounter in real life are multi-dimensional and can’t be plotted on a 2-d plot.)\n",
    "\n",
    "The dataset is loaded from the data file into the variables X and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/ex1data1.txt', delimiter=\",\") # read comma separated data\n",
    "m = data.shape[0]                                     # number of training example\n",
    "X = data[:,0].reshape(m,1)\n",
    "y = data[:,1].reshape(m,1)                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X,y, 'rx')                         # Plot the data\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will fit the linear regression parameters $\\theta$ to our dataset\n",
    "using gradient descent.\n",
    "\n",
    "The objective of linear regression is to minimize the cost function\n",
    "\\begin{equation*}\n",
    "J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^2\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "where the hypothesis $h_\\theta(x)$ is given by the linear mode\n",
    "\\begin{equation*}\n",
    "h_{\\theta}(x^{(i)}) = \\theta^Tx = \\theta_0 + \\theta_1 x_1\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Recall that the parameters of your model are the $\\theta_j$ values. These are\n",
    "the values you will adjust to minimize cost $J(\\theta)$. One way to do this is to\n",
    "use the batch gradient descent algorithm. In batch gradient descent, each\n",
    "iteration performs the update\n",
    "\\begin{equation*}\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)}) x_j^{(i)}\n",
    "\\end{equation*}\n",
    "\n",
    "With each step of gradient descent, your parameters $\\theta_j$ come closer to the\n",
    "optimal values that will achieve the lowest cost $J(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you perform gradient descent to learn minimize the cost function J(θ), it is helpful to monitor the convergence by computing the cost. In this section, you will implement a function to calculate J(θ) so you can check the convergence of your gradient descent implementation.\n",
    "\n",
    "Your next task is to complete the `compute_cost` function, which is a function that computes J(θ). As you are doing this, remember that the variables X and y are not scalar values, but matrices whose rows represent the examples from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    m = len(y); # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0;\n",
    "    #####################################################################\n",
    "    # Compute the cost of a particular choice of theta                  #\n",
    "    #               You should set J to the cost.                       #\n",
    "    #####################################################################\n",
    "    pass\n",
    "    #####################################################################\n",
    "    #                       END OF YOUR CODE                            #\n",
    "    ####################################################################\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((np.ones((m, 1)), data[:,0].reshape(m,1)), axis=1)\n",
    "theta = np.zeros((2, 1)) \n",
    "\n",
    "compute_cost(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect to see a cost of 32.07."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will implement gradient descent function. The loop structure has been written for you, and you only need to supply the updates to θ within each iteration.\n",
    "\n",
    "As you program, make sure you understand what you are trying to optimize and what is being updated. Keep in mind that the cost J(θ) is parameterized by the vector θ, not X and y. That is, we minimize the value of J(θ) by changing the values of the vector θ, not by changing X or y.\n",
    "\n",
    "A good way to verify that gradient descent is working correctly is to look at the value of J(θ) and check that it is decreasing with each step. The starter code calls `compute_cost` on every iteration and prints the cost. Assuming you have implemented gradient descent and `compute_cost` correctly, your value of J(θ) should never increase, and should converge to a steady value by the end of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    # GRADIENTDESCENT Performs gradient descent to learn theta\n",
    "    # theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by \n",
    "    # taking num_iters gradient steps with learning rate alpha\n",
    "\n",
    "    # Initialize some useful values\n",
    "    m = len(y)\n",
    "    J_history = []\n",
    "\n",
    "    \n",
    "    for iter in range(num_iters):\n",
    "\n",
    "        \n",
    "        #####################################################################\n",
    "        # Instructions: Perform a single gradient step on the parameter     #\n",
    "        #               vector theta.                                       #\n",
    "        #                                                                   #      \n",
    "        # Hint: While debugging, it can be useful to print out the values   #\n",
    "        #       of the cost function (compute_cost) and gradient here.       # \n",
    "        #####################################################################\n",
    "        pass\n",
    "        #####################################################################\n",
    "        #                       END OF YOUR CODE                            #\n",
    "        #####################################################################\n",
    "    \n",
    "\n",
    "        # Save the cost J in every iteration \n",
    "        J = compute_cost(X, y, theta)\n",
    "        J_history.append(J)\n",
    "    \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the parameter θ and plot the linear fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running Gradient Descent ...\\n')\n",
    "\n",
    "X = np.concatenate((np.ones((m, 1)), data[:,0].reshape(m,1)), axis=1) # Add a column of ones to x\n",
    "theta = np.zeros((2, 1))                                              # initialize fitting parameters\n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "\n",
    "# gradient descent\n",
    "theta, J_history = gradient_descent(X, y, theta, alpha, iterations)\n",
    "print('Theta found by gradient descent: ')\n",
    "print(theta[0], theta[1])\n",
    "\n",
    "\n",
    "plt.plot(X[:,1], y, 'rx')                         # Plot the data\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "\n",
    "plt.plot(X[:,1], np.dot(X, theta), '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with multiple variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\n",
    "\n",
    "The file ex1data2.txt contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price\n",
    "of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/ex1data2.txt', delimiter=\",\") # read comma separated data\n",
    "m = data.shape[0]                                     # number of training example\n",
    "X = data[:,0:2].reshape(m,2)\n",
    "y = data[:,2].reshape(m,1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalize(X):\n",
    "    \n",
    "    # FEATURENORMALIZE Normalizes the features in X \n",
    "    #   FEATURENORMALIZE(X) returns a normalized version of X where the mean value of each\n",
    "    #   feature is 0 and the standard deviation is 1. This is often a good preprocessing \n",
    "    #   step to do when working with learning algorithms.\n",
    "\n",
    "    # You need to set these values correctly\n",
    "    X_norm = X\n",
    "    mu     = 0\n",
    "    sigma  = 0\n",
    "\n",
    "    #####################################################################\n",
    "    # Instructions: First, for each feature dimension, compute the mean #\n",
    "    #               of the feature and subtract it from the dataset,    #\n",
    "    #               storing the mean value in mu. Next, compute the     #\n",
    "    #               standard deviation of each feature and divide       #\n",
    "    #               each feature by it's standard deviation, storing    #\n",
    "    #               the standard deviation in sigma.                    #\n",
    "    #                                                                   #\n",
    "    #               Note that X is a matrix where each column is a      #\n",
    "    #               feature and each row is an example. You need        #\n",
    "    #               to perform the normalization separately for         #\n",
    "    #               each feature.                                       #\n",
    "    #                                                                   #\n",
    "    # Hint: You might find the 'mean' and 'std' functions useful.       #\n",
    "    #####################################################################\n",
    "    pass\n",
    "    #####################################################################\n",
    "    #                       END OF YOUR CODE                            #\n",
    "    #####################################################################\n",
    "        \n",
    "\n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, you implemented gradient descent on a univariate regression problem. The only difference now is that there is one more feature in the matrix X. The hypothesis function and the batch gradient descent update\n",
    "rule remain unchanged.\n",
    "\n",
    "You should complete the function gradientDescentMulti to implement the gradient descent for linear regression with multiple variables.\n",
    "\n",
    "Make sure your code supports any number of features and is well-vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((np.ones((m, 1)),feature_normalize(data[:,0:2].reshape(m,2))[0]), axis=1)\n",
    "theta = np.zeros((3, 1)) \n",
    "\n",
    "compute_cost(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect to see a cost of 65591548106.\n",
    "\n",
    "Next, you will implement gradient descent function with multiple variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_multi(X, y, theta, alpha, num_iters):\n",
    "    #GRADIENTDESCENTMULTI Performs gradient descent to learn theta\n",
    "    #   theta = GRADIENTDESCENTMULTI(x, y, theta, alpha, num_iters) updates theta by\n",
    "    #   taking num_iters gradient steps with learning rate alpha\n",
    "\n",
    "    # Initialize some useful values\n",
    "    m = len(y)\n",
    "    J_history = []\n",
    "\n",
    "    \n",
    "    for iter in range(num_iters):\n",
    "\n",
    "        \n",
    "        #####################################################################\n",
    "        # Instructions: Perform a single gradient step on the parameter     #\n",
    "        #               vector theta.                                       #\n",
    "        #                                                                   #      \n",
    "        # Hint: While debugging, it can be useful to print out the values   #\n",
    "        #       of the cost function (compute_cost) and gradient here.      # \n",
    "        #####################################################################\n",
    "        pass \n",
    "        #####################################################################\n",
    "        #                       END OF YOUR CODE                            #\n",
    "        #####################################################################\n",
    "    \n",
    "\n",
    "        # Save the cost J in every iteration \n",
    "        J = compute_cost(X, y, theta)\n",
    "        print(J)\n",
    "        J_history.append(J)\n",
    "    \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the parameter θ and plot the linear fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01;\n",
    "num_iters = 400;\n",
    "\n",
    "theta = np.zeros((3, 1))\n",
    "theta, J_history = gradient_descent_multi(X, y, theta, alpha, num_iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the convergence graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(0, len(J_history))), J_history, '-b')                         # Plot the data\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost J')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3. Logistic Regression [25 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following logistic regression assignment is modified from [Stanford CS229](http://cs229.stanford.edu//). Please complete and hand in this completed worksheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "In this section, you need to implement logsitic regression to solve a binary classification problem. Let's first get our data ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use the first 70 samples for training (and validation),\n",
    "# and treat the rest of them as hold-out testing set.\n",
    "X = np.loadtxt('data/logistic_x_.txt') \n",
    "y = np.loadtxt('data/logistic_y_.txt').reshape(-1, 1) \n",
    "\n",
    "\n",
    "X, mu, std = feature_normalize(X)\n",
    "\n",
    "# Add a column of ones to X for the bias weight.\n",
    "m = len(X)\n",
    "X = np.concatenate((np.ones((m, 1)), X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the input $x^{(i)}\\in\\mathbb{R^2}$ and $y^{(i)}\\in\\{-1, 1\\}$. Like we have mentioned, it is better to visualize the data first before you start working on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature according to their class label.\n",
    "# Note that we exclude column 0, which is the colunm we padded with one in the previous block.\n",
    "plt.plot(X[np.where(y==1), 1], X[np.where(y==1), 2], 'rx')\n",
    "plt.plot(X[np.where(y==-1), 1], X[np.where(y==-1), 2], 'bo')  \n",
    "plt.xlabel('x2')\n",
    "plt.ylabel('x1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, you need to implement logistic regression. Recall that when $y^{(i)}\\in{-1,1}$, the objective function for binary logistic regression can be expressed as:\n",
    "\\begin{equation*}\n",
    "J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\log{\\left(1+e^{-y^{(i)\\theta^Tx^{(i)}}}\\right)}=-\\frac{1}{m}\\sum_{i=1}^m\\log{\\left(h_{\\theta}(y^{(i)}x^{(i)})\\right)}\n",
    "\\end{equation*}\n",
    "where the hypothesis is the **sigmoid function**: \n",
    "\\begin{equation*}\n",
    "h_\\theta(y^{(i)}x^{(i)})=\\frac{1}{1+e^{-y^{(i)}\\theta^{T}x^{(i)}}}\n",
    "\\end{equation*}\n",
    "which we have seen in class (and assignment 0). Similar to the previous section, we can minimize the objective function $J(\\theta)$ using  batch gradient descent:\n",
    "\\begin{equation*}\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}h_\\theta(-y^{(i)}x_j^{(i)})(-y^{(i)}x_j^{(i)})\n",
    "\\end{equation*}\n",
    "\n",
    "Now, your task is to complete the function `sigmoid`, `compute_cost`, `gradient_descent` for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    #####################################################################\n",
    "    # Instructions: Implement sigmoid function g                        #\n",
    "    #####################################################################\n",
    "    pass\n",
    "    #####################################################################\n",
    "    #                       END OF YOUR CODE                            #\n",
    "    #####################################################################\n",
    "    return g\n",
    "\n",
    "def compute_cost(X, y, theta):\n",
    "    \n",
    "    # You need to return the following variables correctly \n",
    "    J = 0;\n",
    "    #####################################################################\n",
    "    # Instructions: Implement the objective function J(theta)           #\n",
    "    #####################################################################\n",
    "    pass\n",
    "    #####################################################################\n",
    "    #                       END OF YOUR CODE                            #\n",
    "    #####################################################################\n",
    "    return J\n",
    "\n",
    "def compute_gradient(X, y, theta):\n",
    "    #####################################################################\n",
    "    # Instructions: Implement gradient function gradient_               #\n",
    "    #####################################################################\n",
    "    pass\n",
    "    #####################################################################\n",
    "    #                       END OF YOUR CODE                            #\n",
    "    #####################################################################\n",
    "    return gradient_\n",
    "\n",
    "\n",
    "def gradient_descent_logistic(X, y, theta, alpha, num_iters):\n",
    "    m = len(y)\n",
    "    J_history = []\n",
    "    for iter in range(num_iters):\n",
    "\n",
    "        \n",
    "        #####################################################################\n",
    "        # Instructions: Perform a single gradient step on the parameter     #\n",
    "        #               vector theta using the implemented compute_gradient #\n",
    "        #                                                                   #      \n",
    "        # Hint: While debugging, it can be useful to print out the values   #\n",
    "        #       of the cost function (compute_cost) and gradient here.      # \n",
    "        #####################################################################\n",
    "        pass\n",
    "        #####################################################################\n",
    "        #                       END OF YOUR CODE                            #\n",
    "        #####################################################################\n",
    "    \n",
    "\n",
    "        # Save the cost J in every iteration \n",
    "        J = compute_cost(X, y, theta)\n",
    "        print(J)\n",
    "        J_history.append(J)\n",
    "    \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fit your model, and see if it is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model.\n",
    "theta = np.zeros((X.shape[1], 1))\n",
    "alpha = 0.1;\n",
    "num_iters = 400;\n",
    "theta, J_history = gradient_descent_logistic(X, y, theta, alpha, num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, plot and check to see if the model is converging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(0, len(J_history))), J_history, '-b')  \n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost J')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundary\n",
    "In addition to checking convergence graph and accuracy, we can also plot out the decision boundary to see what does the model actually learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature according to their class label.\n",
    "# Note that we exclude column 0, which is the colunm we padded with one in the previous block.\n",
    "plt.plot(X[np.where(y==1), 1], X[np.where(y==1), 2], 'rx')\n",
    "plt.plot(X[np.where(y==-1), 1], X[np.where(y==-1), 2], 'bo')\n",
    "\n",
    "#####################################################################\n",
    "# Instructions: Plot out the decision boundary.                     #\n",
    "# Hint: To plot the boundary, which is a straight line in our case, #\n",
    "#       you need to find the two ends of the line, and plot it with #\n",
    "#       plt.plot(). Note that the decision boundary is the line that#\n",
    "#       y = 0.                                                      # \n",
    "#####################################################################\n",
    "pass\n",
    "#####################################################################\n",
    "#                       END OF YOUR CODE                            #\n",
    "#####################################################################\n",
    "\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4. Regularization [25 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you need to incorporate L2 regularization into your logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization\n",
    "Overfitting is a notorious problem in the world of machine learning. One simple way to counter this issue is to put constraints on your model weights $\\theta$, as we have discussed in class. In this section, you need to modify the the objective function to impose L2 regularization on the logistic regression:\n",
    "\\begin{equation*}\n",
    "    J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m\\log{\\left(h_{\\theta}(y^{(i)}x^{(i)})\\right)} + \\lambda\\vert\\vert\\theta\\vert\\vert_2^2\n",
    "\\end{equation*}\n",
    "Derive the gradient for this new objective to incorporate it into your logistic regression model.\n",
    "\n",
    "To make things much structural, we now put everything together into a class. Please use the class template below to implement your logistic regression. Note that you can add your own class methods if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    \n",
    "    def __init__(self, alpha=0.1, lamb=0.1, regularization=None):\n",
    "        # setting the class attribute.\n",
    "        self.alpha = alpha                   # Set up your learning rate alpha.\n",
    "        self.lamb = lamb                     # Strength of regularization.\n",
    "        self.regularization = regularization \n",
    "        assert regularization == 'l2' or regularization == None # we only consider these two cases\n",
    "    \n",
    "    def _compute_cost(self, X, y):\n",
    "        #####################################################################\n",
    "        # Instructions: Compute the cost function here.                     #\n",
    "        #               You need to handle both the cases with, and without #\n",
    "        #               regularization here.                                #\n",
    "        #####################################################################\n",
    "        pass\n",
    "        #####################################################################\n",
    "        #                       END OF YOUR CODE                            #\n",
    "        #####################################################################\n",
    "        return J\n",
    "        \n",
    "    def _compute_gradient(self, X, y):\n",
    "        #####################################################################\n",
    "        # Instructions: Compute the gradient here.                          #\n",
    "        #               You need to handle both the cases with, and without #\n",
    "        #               regularization here.                                #\n",
    "        #####################################################################\n",
    "        pass\n",
    "        #####################################################################\n",
    "        #                       END OF YOUR CODE                            #\n",
    "        #####################################################################\n",
    "        return gradient\n",
    "\n",
    "    def fit(self, X, y, num_iter=5):\n",
    "        self.theta = np.zeros((X.shape[1], 1))\n",
    "        m = len(y)\n",
    "        J_history = []\n",
    "        #####################################################################\n",
    "        # Instructions: Run the gradient decsent here.                      #\n",
    "        #####################################################################\n",
    "        pass\n",
    "        #####################################################################\n",
    "        #                       END OF YOUR CODE                            #\n",
    "        #####################################################################\n",
    "        return J_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #####################################################################\n",
    "        # Instructions: Use your hypothese to make predictions.             #\n",
    "        #####################################################################\n",
    "        pass\n",
    "        #####################################################################\n",
    "        #                       END OF YOUR CODE                            #\n",
    "        #####################################################################\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the wine datasets, in which $x_j\\in\\mathbb{R}^{12}$ is different attribute for alcohol, and $y\\in\\{-1,1\\}$ is that class label (red or white wine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "X_train = np.loadtxt('data/wine_train_X.txt')\n",
    "y_train = np.loadtxt('data/wine_train_y.txt').reshape(-1, 1)\n",
    "X_test = np.loadtxt('data/wine_test_X.txt')\n",
    "y_test = np.loadtxt('data/wine_test_y.txt').reshape(-1, 1)\n",
    "\n",
    "\n",
    "X_train = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis=1)\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train two different logistic regression models: one with, and one without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(alpha=0.1) # Without regularization\n",
    "log_reg_l2 = LogisticRegression(alpha=0.1, lamb=1.0, regularization='l2') # Without regularization\n",
    "\n",
    "J_history = log_reg.fit(X_train, y_train, num_iter=500)\n",
    "J_history_l2 = log_reg_l2.fit(X_train, y_train, num_iter=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we evaluate the accuracy for each method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(X, y, model):\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred[y_pred > 0.5] = 1\n",
    "    y_pred[y_pred <= 0.5] = -1\n",
    "    return np.mean(y_pred == y)\n",
    "\n",
    "print(\"Accuracy on training set: \", evaluate_accuracy(X_train, y_train, log_reg))\n",
    "print(\"Accuracy on testing set: \", evaluate_accuracy(X_test, y_test, log_reg))\n",
    "print(\"Accuracy w/ L2 training set: \", evaluate_accuracy(X_train, y_train, log_reg_l2))\n",
    "print(\"Accuracy w/ L2 testing set: \", evaluate_accuracy(X_test, y_test, log_reg_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the effect of regularization on $\\theta$, we can plot out each $\\theta_j$ under different $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_theta(theta, lamb):\n",
    "    \"\"\"\n",
    "    Helper function for plotting out the value of theta with respect to different lambda.\n",
    "    theta  (list): list of theta under different lambda.\n",
    "    lambda (list): list of lambda values you tried.\n",
    "    \"\"\"\n",
    "    plt.hlines(y=0, xmin=0, xmax=np.max(lamb), color='red', linewidth = 2, linestyle = '--')\n",
    "    for i in range(theta.shape[1]):\n",
    "        plt.plot(lamb, theta[:,i])\n",
    "    plt.ylabel('theta')\n",
    "    plt.xlabel('lambda')\n",
    "    plt.xscale('log')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = [0.1, 1, 10, 100, 1000]\n",
    "theta = []\n",
    "\n",
    "#####################################################################\n",
    "# Instructions: For each value in lamb, try a model for it, and     #\n",
    "#               append the trained weights into the theta           #\n",
    "#####################################################################\n",
    "pass\n",
    "#####################################################################\n",
    "#                       END OF YOUR CODE                            #\n",
    "#####################################################################\n",
    "\n",
    "plot_theta(np.array(theta), lamb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
